{
	"name": "Homework 6",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "openweather",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c7729094-58c3-43d6-81c8-20acde914268"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5d043d5b-a1ca-4e81-88e6-fac0964cefde/resourceGroups/DS598-Anthony-Huang/providers/Microsoft.Synapse/workspaces/anthonyhuangsynapse/bigDataPools/openweather",
				"name": "openweather",
				"type": "Spark",
				"endpoint": "https://anthonyhuangsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/openweather",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Set up the configuration for accessing the storage account\n",
					"storage_account_name = \"ds598anthonyhuangstorage\"\n",
					"storage_account_key = \"\"\n",
					"\n",
					"spark.conf.set(\n",
					"    \"fs.azure.account.key.\" + storage_account_name + \".dfs.core.windows.net\",\n",
					"    storage_account_key\n",
					")\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"# Read data directly from ADLS Gen2\n",
					"container_name = \"ds598anthonyhuangblobcontainer\"\n",
					"air_pollution_folder_path = \"silver/pollution_silver\"\n",
					"historical_weather_folder_path = \"silver/historical_weather_silver\"\n",
					"\n",
					"# Read all Parquet files in the folder\n",
					"air_pollution_df = spark.read.parquet(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{air_pollution_folder_path}\")\n",
					"weather_df = spark.read.parquet(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{historical_weather_folder_path}\")\n",
					"# Show the DataFrame"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"source": [
					"import pandas as pd\n",
					"from pyspark.sql.functions import col\n",
					"import pyspark.sql.functions as F\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.types import IntegerType"
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"source": [
					"def calculate_aqi(pollutant, concentration):\n",
					"    breakpoints = {\n",
					"        'O3_8hr': [(0, 54, 0, 50), (55, 70, 51, 100), (71, 85, 101, 150), (86, 105, 151, 200), (106, 200, 201, 300)],\n",
					"        'O3_1hr': [(125, 164, 101, 150), (165, 204, 151, 200), (205, 404, 201, 300), (405, 504, 301, 400), (505, 604, 401, 500)],\n",
					"        'PM2.5_24hr': [(0, 12, 0, 50), (12.1, 35.4, 51, 100), (35.5, 55.4, 101, 150), (55.5, 150.4, 151, 200), (150.5, 250.4, 201, 300), (250.5, 350.4, 301, 400), (350.5, 500.4, 401, 500)],\n",
					"        'PM10_24hr': [(0, 54, 0, 50), (55, 154, 51, 100), (155, 254, 101, 150), (255, 354, 151, 200), (355, 424, 201, 300), (425, 504, 301, 400), (505, 604, 401, 500)],\n",
					"        'CO_8hr': [(0, 4.4, 0, 50), (4.5, 9.4, 51, 100), (9.5, 12.4, 101, 150), (12.5, 15.4, 151, 200), (15.5, 30.4, 201, 300), (30.5, 40.4, 301, 400), (40.5, 50.4, 401, 500)],\n",
					"        'SO2_1hr': [(0, 35, 0, 50), (36, 75, 51, 100), (76, 185, 101, 150), (186, 304, 151, 200)],\n",
					"        'SO2_24hr': [(305, 604, 201, 300), (605, 804, 301, 400), (805, 1004, 401, 500)],\n",
					"        'NO2_1hr': [(0, 53, 0, 50), (54, 100, 51, 100), (101, 360, 101, 150), (361, 649, 151, 200), (650, 1249, 201, 300), (1250, 2049, 301, 400), (2050, 3049, 401, 500)],\n",
					"    }\n",
					"\n",
					"    if pollutant not in breakpoints:\n",
					"        raise ValueError(f\"Unsupported pollutant: {pollutant}\")\n",
					"\n",
					"    for (Clow, Chigh, Ilow, Ihigh) in breakpoints[pollutant]:\n",
					"        if Clow <= concentration <= Chigh:\n",
					"            return round(((Ihigh - Ilow) / (Chigh - Clow)) * (concentration - Clow) + Ilow)\n",
					"\n",
					"    return None  # If the concentration is out of the given ranges"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"source": [
					"def calculate_rolling_average(df, column, window):\n",
					"    windowSpec = Window.partitionBy(\"location\").orderBy(\"date_time\").rowsBetween(-window+1, 0)\n",
					"    return F.avg(column).over(windowSpec)\n",
					"\n",
					"def calculate_aqi_row(row):\n",
					"    pollutant_map = {\n",
					"        'o3_8hr': 'O3_8hr',\n",
					"        'o3_1hr': 'O3_1hr',\n",
					"        'pm2_5_24hr': 'PM2.5_24hr',\n",
					"        'pm10_24hr': 'PM10_24hr',\n",
					"        'co_8hr': 'CO_8hr',\n",
					"        'so2_1hr': 'SO2_1hr',\n",
					"        'so2_24hr': 'SO2_24hr',\n",
					"        'no2_1hr': 'NO2_1hr'\n",
					"    }\n",
					"    aqi_values = []\n",
					"\n",
					"    for col_name, pollutant in pollutant_map.items():\n",
					"        concentration = row[col_name]\n",
					"        if concentration is not None:\n",
					"            aqi = calculate_aqi(pollutant, concentration)\n",
					"            if aqi is not None:\n",
					"                aqi_values.append(aqi)\n",
					"\n",
					"    if aqi_values:\n",
					"        return max(aqi_values)\n",
					"    return None\n",
					"\n",
					"def calculate_us_aqi(df):\n",
					"    df = df.withColumn(\"o3_8hr\", calculate_rolling_average(df, \"o3\", 8))\n",
					"    df = df.withColumn(\"o3_1hr\", df[\"o3\"])\n",
					"    df = df.withColumn(\"pm2_5_24hr\", calculate_rolling_average(df, \"pm2_5\", 24))\n",
					"    df = df.withColumn(\"pm10_24hr\", calculate_rolling_average(df, \"pm10\", 24))\n",
					"    df = df.withColumn(\"co_8hr\", calculate_rolling_average(df, \"co\", 8))\n",
					"    df = df.withColumn(\"so2_1hr\", df[\"so2\"])\n",
					"    df = df.withColumn(\"so2_24hr\", calculate_rolling_average(df, \"so2\", 24))\n",
					"    df = df.withColumn(\"no2_1hr\", df[\"no2\"])\n",
					"\n",
					"    calculate_aqi_udf = F.udf(lambda row: calculate_aqi_row(row), IntegerType())\n",
					"    \n",
					"    df = df.withColumn(\"us_aqi\", calculate_aqi_udf(F.struct(\n",
					"        col(\"o3_8hr\"),\n",
					"        col(\"o3_1hr\"),\n",
					"        col(\"pm2_5_24hr\"),\n",
					"        col(\"pm10_24hr\"),\n",
					"        col(\"co_8hr\"),\n",
					"        col(\"so2_1hr\"),\n",
					"        col(\"so2_24hr\"),\n",
					"        col(\"no2_1hr\")\n",
					"    )))\n",
					"\n",
					"    return df\n",
					"\n",
					"air_pollution_df = calculate_us_aqi(air_pollution_df)"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"source": [
					"air_pollution_df.select(\n",
					"    \"date_time\", \n",
					"    \"o3_8hr\", \n",
					"    \"o3_1hr\", \n",
					"    \"pm2_5_24hr\", \n",
					"    \"pm10_24hr\", \n",
					"    \"co_8hr\", \n",
					"    \"so2_1hr\", \n",
					"    \"so2_24hr\", \n",
					"    \"no2_1hr\", \n",
					"    \"us_aqi\",\n",
					"    \"aqi\"\n",
					").limit(10).toPandas()"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"source": [
					"# Aggregation by Date: Compute daily averages for temperature, humidity, wind speed, etc.\n",
					"weather_agg_df = weather_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
					"    F.round(F.avg(\"temp_F\")).alias(\"avg_temp_F\"),\n",
					"    F.round(F.avg(\"humidity\"),2).alias(\"avg_humidity\"),\n",
					"    F.round(F.avg(\"wind_speed\"),2).alias(\"avg_wind_speed\"),\n",
					"    F.round(F.max(\"temp_max_F\")).alias(\"max_temp_F\"),\n",
					"    F.round(F.min(\"temp_min_F\")).alias(\"min_temp_F\"),\n",
					"    F.count(\"id\").alias(\"weather_records\")\n",
					")\n",
					"\n",
					"# Weather Condition Counts: Count the occurrences of different weather conditions for each day\n",
					"weather_condition_counts_df = weather_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\"), \"weather_main_value\").count()\n",
					"\n",
					"# Temperature Extremes: Identify the maximum and minimum temperatures for each day\n",
					"temp_extremes_df = weather_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
					"    F.max(\"temp_max_F\").alias(\"max_temp_F\"),\n",
					"    F.min(\"temp_min_F\").alias(\"min_temp_F\")\n",
					")"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"source": [
					"weather_agg_df.select(\n",
					"    \"date\",\n",
					"    \"avg_temp_F\",\n",
					"    \"avg_humidity\",\n",
					"    \"avg_wind_speed\",\n",
					"    \"max_temp_F\",\n",
					"    \"min_temp_F\",\n",
					"    \"weather_records\").limit(10).toPandas()"
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"source": [
					"weather_condition_counts_df.select(\n",
					"    \"date\",\n",
					"    \"weather_main_value\",\n",
					"    \"count\").limit(10).toPandas()"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"source": [
					"temp_extremes_df.select(\n",
					"    \"date\",\n",
					"    \"max_temp_F\",\n",
					"    \"min_temp_F\").limit(10).toPandas()"
				],
				"execution_count": 45
			},
			{
				"cell_type": "code",
				"source": [
					"# Daily Average AQI: Compute the daily average Air Quality Index (AQI)\n",
					"aqi_agg_df = air_pollution_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
					"    F.round(F.avg(\"us_aqi\")).alias(\"avg_us_aqi\")\n",
					")\n",
					"\n",
					"# Pollutant Aggregation: Calculate daily averages for each pollutant\n",
					"pollutant_agg_df = air_pollution_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
					"    F.round(F.avg(\"co\"),2).alias(\"avg_co\"),\n",
					"    F.round(F.avg(\"no2\"),2).alias(\"avg_no2\"),\n",
					"    F.round(F.avg(\"o3\"),2).alias(\"avg_o3\"),\n",
					"    F.round(F.avg(\"so2\"),2).alias(\"avg_so2\"),\n",
					"    F.round(F.avg(\"pm2_5\"),2).alias(\"avg_pm2_5\"),\n",
					"    F.round(F.avg(\"pm10\"),2).alias(\"avg_pm10\")\n",
					")\n",
					"\n",
					"# Identify High Pollution Events: Mark days with high pollution levels based on a specified threshold\n",
					"high_pollution_events_df = air_pollution_df.withColumn(\"high_pollution\", F.when(col(\"us_aqi\") > 100, 1).otherwise(0)).groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
					"    F.sum(\"high_pollution\").alias(\"high_pollution_events\")\n",
					")"
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"source": [
					"aqi_agg_df.select(\n",
					"    \"date\",\n",
					"    \"avg_us_aqi\").limit(10).toPandas()"
				],
				"execution_count": 46
			},
			{
				"cell_type": "code",
				"source": [
					"pollutant_agg_df.select(\n",
					"    \"date\",\n",
					"    \"avg_co\",\n",
					"    \"avg_no2\",\n",
					"    \"avg_o3\",\n",
					"    \"avg_so2\",\n",
					"    \"avg_pm2_5\",\n",
					"    \"avg_pm10\").limit(10).toPandas()"
				],
				"execution_count": 47
			},
			{
				"cell_type": "code",
				"source": [
					"high_pollution_events_df.select(\n",
					"    \"date\",\n",
					"    \"high_pollution_events\").limit(10).toPandas()"
				],
				"execution_count": 48
			},
			{
				"cell_type": "code",
				"source": [
					"abfss_base_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"# Save aggregated data as single files\n",
					"weather_agg_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{abfss_base_path}/gold/agg_weather\")\n",
					"weather_condition_counts_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{abfss_base_path}/gold/agg_weather_conditions\")\n",
					"temp_extremes_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{abfss_base_path}/gold/agg_temp_extremes\")\n",
					"aqi_agg_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{abfss_base_path}/gold/agg_aqi\")\n",
					"pollutant_agg_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{abfss_base_path}/gold/agg_pollutants\")\n",
					"high_pollution_events_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{abfss_base_path}/gold/agg_high_pollution_events\")\n",
					"\n",
					"# Save processed weather data\n",
					"weather_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{abfss_base_path}/gold/processed_weather\")\n",
					"\n",
					"# Save processed air pollution data\n",
					"air_pollution_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{abfss_base_path}/gold/processed_air_pollution\")\n",
					""
				],
				"execution_count": 52
			}
		]
	}
}